import streamlit as st
from PIL import Image
import torch
from transformers import AutoProcessor, AutoModelForCausalLM
import json
import html
import supervision as sv
import base64
import io
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
MODEL_ID = '/home/nick/deeplearning/DIPLOMA/project/plantato/checkpoints/var_2'
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

@st.cache_resource
def load_model():
    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, trust_remote_code=True).to(DEVICE)
    return processor, model

processor, model = load_model()

def plot_bbox(image, data):
   # Create a figure and axes
    fig, ax = plt.subplots()

    # Display the image
    ax.imshow(image)

    # Plot each bounding box
    for bbox, label in zip(data['bboxes'], data['labels']):
        # Unpack the bounding box coordinates
        x1, y1, x2, y2 = bbox
        # Create a Rectangle patch
        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')
        # Add the rectangle to the Axes
        ax.add_patch(rect)
        # Annotate the label
        plt.text(x1, y1, label, color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))

    # Remove the axis ticks and labels
    ax.axis('off')

    # Show the plot
    return plt

def render_results(image: Image.Image, response):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å bounding boxes"""
    try:
        detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)
        image = sv.BoxAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image.copy(), detections)
        image = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image, detections)
        return image
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        return image
    
# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å
st.set_page_config(page_title="Plant Disease Detection", page_icon="üåø")
st.title("üå± –ê–Ω–∞–ª–∏–∑ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π –ª–∏—Å—Ç—å–µ–≤")
uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ª–∏—Å—Ç–∞", type=["jpg", "png", "jpeg"])

if uploaded_file:
    image = Image.open(uploaded_file).convert("RGB")
    st.image(image, caption="–ò—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", use_column_width=True)

    if st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å"):
        with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞..."):
            # –ö–ª—é—á–µ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: –ø–µ—Ä–µ–¥–∞–µ–º –¢–û–õ–¨–ö–û <OD> –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            inputs = processor(text="<OD>", images=image, return_tensors="pt").to(DEVICE)
            
            with torch.no_grad():
                generated_ids = model.generate(
                    input_ids=inputs["input_ids"],
                    pixel_values=inputs["pixel_values"],
                    max_new_tokens=1024,
                    num_beams=3
                )
            
            generated_text = processor.batch_decode(generated_ids, skip_special_ytokens=False)[0]
            result = processor.post_process_generation(generated_text, task='<OD>', image_size=image.size)
        
        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(result)
        if isinstance(result, dict) and len(result['<OD>']['bboxes']) != 0:
            annotated_img = render_results(image, result)
            st.image(annotated_img, caption="–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è", use_column_width=True)
            st.json(result['<OD>']['labels'])  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º raw-–æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        else:
            st.warning("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∞ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π.")